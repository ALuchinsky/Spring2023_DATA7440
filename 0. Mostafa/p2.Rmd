---
title: "p2"
author: "Mostafa"
date: "04/25/2023"
output:
  html_notebook:
    toc: TRUE
    toc_float:
      collapsed: FALSE
---

# Problem 2    

[50 points]

We will use the api data again for this problem, but this time we are interested in predicting the api test scores for 2000 (i.e. api00) based on a battery of school, teacher and parent information. This time the data and models are available in the apipopProb2.RData R workspace. Note the data sets here are different than for Problem 1.   

For this problem, you are given three models that aim at predicting **api00** (R objects model1 â€“ model3), using different methods (i.e., CART, XGBoost, OLS regression). Utilize the following interpretation techniques with the training data to learn more about how those models were trained. Specifically for each of the models you should:   

## Libraries

```{r}
rm(list = ls())

suppressPackageStartupMessages(library(randomForest)) # Random Forest
suppressPackageStartupMessages(library(pdp)) # pdp
suppressPackageStartupMessages(library(ggplot2)) # ggplot2
suppressPackageStartupMessages(library(iml)) # iml
suppressPackageStartupMessages(library(kableExtra)) # kableExtra
suppressPackageStartupMessages(library(knitr)) # needed for the kable function
suppressPackageStartupMessages(library(ICEbox))

suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(grid))
suppressPackageStartupMessages(library(gridExtra))
suppressPackageStartupMessages(library(lattice))
suppressPackageStartupMessages(library(xgboost))
suppressPackageStartupMessages(library(glmnet))
suppressPackageStartupMessages(library(rpart))
```

## Dataset

```{r}
load("./data/external/apipopProb2.RData")
```

## Functions

### F1. PDP1var    

```{r}
PDP1var <- function(model1, var1) {

    pd1 <- partial(model1, 
                   pred.var = var1, 
                   plot = TRUE, 
                   rug = TRUE
                   ) 
    
    pd1a <- partial(model1, 
                    pred.var = var1, 
                    plot = TRUE, 
                    plot.engine="ggplot2"
                    ) 
    
    pd1a <- pd1a + ggtitle("ggplot2-based PDP") 
    
    grid.arrange(pd1, 
                 pd1a, 
                 ncol=2
                 )

}
```

### F2. PDP2var    

```{r}
PDP2var <- function(model1, var1, var2) {
    
    pdp12 <-  partial(model1, 
                      pred.var = c(var1, var2), 
                      plot = TRUE, 
                      chull = TRUE, 
                      palette = "magma"
                      )
    pdp12
}
```

### F3. ALE     

```{r}
ALE <- function(model1, feature1, data1, response1){
  
    predictor1 <- Predictor$new(model = model1, 
                                data = data1, 
                                y = response1
                                )
    
    pdp1  <- FeatureEffect$new(predictor1, 
                               feature=feature1, 
                               method="pdp"
                               ) 
    
    ale1  <- FeatureEffect$new(predictor1, 
                               feature=feature1
                               ) 
    
    grid.arrange(ale1$plot(), 
                 pdp1$plot(), 
                 ncol=2
                 )
}
```

### F4. Interaction overall   

```{r}
Interaction_overall <- function(model1, data1, response1) {
    
    predictor1 <- Predictor$new(model1, 
                                data = data1, 
                                y = response1
                                )
    interact <- Interaction$new(predictor1)
    results_head <- head(interact$results, nrow(interact$results))
    plot_output <- plot(interact)
    
    output_list <- list(results_head = results_head, 
                        plot_output = plot_output
                        )
    return(output_list)
}
```

### F5. Interaction feature spec

```{r}
Interaction_feature_specific <- function(model1, data1, response1, feature1) {
    
    predictor1 <- Predictor$new(model1, 
                                data = data1, 
                                y = response1
                                )
    interact <- Interaction$new(predictor1,
                                feature = feature1
                                )
    results_head <- head(interact$results, nrow(interact$results))
    plot_output <- plot(interact)
    
    output_list <- list(results_head = results_head, 
                        plot_output = plot_output
                        )
    return(output_list)
}
```

## Important predictor

### model 1 VarImp

```{r}
varImp(model1, scale = FALSE)
```

### model 2 VarImp

```{r}
varImp(model2, scale = FALSE)
```

### model 3 VarImp

```{r}
varImp(model3, scale = FALSE)
```

## a. PDP 1-var 

PDP 1-var: Partial Dependence plots 1-var   

Produce Partial Dependence plots for the most important predictor of each model.

### model1: GLM

GLM: Generalized Linear Model

**Interpretation**:      
A steep straight decline which means a strong negative relationship between "yhat" and "meals".

```{r}
PDP1var(model1, "meals")
```

### model2: CART

CART: Classification And Regression Tree 

**Interpretation**:      

The curve on the Partial Dependence Plot (PDP) appears as a step function, with flat regions separated by sharp jumps downward, it suggests that there may be specific values of the predictor variable (not.hsg) that are particularly important in determining the predicted outcome (yhat).    

```{r}
PDP1var(model2, "not.hsg")
```

### model3: XGBoost

XGBoost: eXtreme Gradient Boosting

**Interpretation**:      

The curve on a Partial Dependence Plot (PDP) is declining overall but has local ups and downs, it suggests:    

1. There may be a non-linear relationship between the predictor variable and the predicted outcome. These local deviations could represent interactions between the predictor variable and other features in the model, or they could suggest that there are specific ranges of the predictor variable where it has a stronger or weaker impact on the predicted outcome.     

2. It's also possible that the local ups and downs in the curve are simply due to noise in the data or limitations of the model. In this case, it's important to carefully evaluate the quality of the data and the appropriateness of the modeling approach to ensure that the PDP accurately represents the relationship between the predictor variable and the predicted outcome.   

```{r}
PDP1var(model3, "meals")
```

## b. PDP 2-var

PDP 2-var: Partial Dependence Plots 2-var  

Produce Partial Dependence plots for the two most important predictors of each model (i.e., 3D plots or heatmaps or contour plots).

### model1: GLM 

GLM: Generalized Linear Model

**Interpretation**:      

This type of relationship is known as a U-shaped relationship, where the predicted outcome is highest when both variables are at low levels, decreases as one or both variables increase, and then may increase again at higher levels of both variables. It suggests that there may be a nonlinear relationship between the two predictor variables and the target variable.    

The slightly increasing slope of the plot suggests that the effect of the predictor variables on the predicted outcome is becoming slightly stronger as the values of the predictor variables increase. However, the U-shaped relationship suggests that there may be other factors at play that are causing the decrease in the predicted outcome as the values of the predictor variables increase.   

```{r}
PDP2var(model1, "meals", "ell")
```

### model2: CART

CART: Classification And Regression Tree

**Interpretation**:      

The decreasing slope suggests that as one predictor variable increases, the effect of the other predictor variable on the predicted outcome is becoming weaker. This is indicative of an interaction effect, where the effect of one predictor variable depends on the value of the other predictor variable.     

The fact that the z-axis is highest when the y-axis is high and the x-axis is low, and lowest when the y-axis is low and the x-axis is high, suggests that the two predictor variables are working in opposite directions. This means that when one predictor variable is high and the other is low, the predicted outcome is high, while the predicted outcome is low when the values are reversed.     

```{r}
PDP2var(model1, "not.hsg", "avg.ed")
```

### model3: XGBoost

XGBoost: eXtreme Gradient Boosting   

**Interpretation**:      

The slightly decreasing slope suggests that the interaction effect is weaker than in the case where the slope is strictly decreasing. However, it still suggests that as one predictor variable increases, the effect of the other predictor variable on the predicted outcome is becoming weaker.     

The fact that the z-axis is highest when the y-axis is high and the x-axis is low, and lowest when the y-axis is low and the x-axis is high, suggests that the two predictor variables are working in opposite directions. This means that when one predictor variable is high and the other is low, the predicted outcome is high, while the predicted outcome is low when the values are reversed.    

```{r}
PDP2var(model1, "meals", "avg.ed")
```

## c. ALE plots

ALE plots: Accumulated Local Effects plots

- Produce Accumulated Local Effects plots for the most important predictor in each model.     
- How do these plots vary from those produced in part b? Given the correlation structure in the apipop_train data is it surprising or not that these plots differ or are similar to each other.
  - **Answer**: 
    - **model1 (GLM)**: The first and second interpretations are describing different types of relationships between predictor variables and a target variable. The first interpretation describes a U-shaped relationship, where the predicted outcome initially increases as both predictor variables increase, but then decreases again at higher levels of both variables. This suggests a nonlinear relationship between the predictor variables and the target variable. In contrast, the second interpretation describes a negative relationship between the predictor variable and the predicted outcome. As the value of the predictor variable increases, the predicted outcome decreases. This suggests a linear relationship between the predictor variable and the target variable. Additionally, the first interpretation mentions that there may be other factors at play causing the decrease in the predicted outcome as the values of the predictor variables increase, while the second interpretation does not mention any potential confounding factors.
    
    - **model2 (CART)**: The two interprets describe different patterns in the relationship between predictor variables and the predicted outcome. The first interpret suggests that there is an interaction effect, where the effect of one predictor variable depends on the value of the other predictor variable. This means that the relationship between the predictor variables and the predicted outcome is not independent but rather depends on the joint values of the predictor variables. Additionally, the fact that the z-axis is highest when the y-axis is high and the x-axis is low, and lowest when the y-axis is low and the x-axis is high, suggests that the two predictor variables are working in opposite directions. On the other hand, the second interpret suggests that the relationship between the predictor variable and the predicted outcome is not linear. There may be discrete intervals or steps where the effect of the predictor variable on the predicted outcome changes. This means that the relationship between the predictor variable and the predicted outcome may be due to different factors or conditions that affect the outcome, such as different subgroups or categories of observations that have different relationships between the predictor variable and the predicted outcome. Therefore, the first interpret suggests an interaction effect between the predictor variables, while the second interpret suggests non-linearity in the relationship between the predictor variable and the predicted outcome.    
    
    - **model3 (XGBoost)**: The first and second interprets are different in their focus and conclusions. The first interpretation describes a linear relationship between two predictor variables and the predicted outcome. It suggests that as one predictor variable increases, the effect of the other predictor variable on the predicted outcome is becoming weaker. It also suggests that the two predictor variables are working in opposite directions, with high values of one variable and low values of the other leading to high predicted outcomes. This interpretation does not suggest any non-monotonic behavior or complex interactions between the predictor variables and other variables in the model. On the other hand, the second interpretation suggests a non-linear and potentially complex relationship between the predictor variable and the predicted outcome. It describes ups and downs in the ALE plot, indicating that the relationship between the predictor variable and the predicted outcome may change direction or magnitude at certain values of the predictor variable. This behavior could arise due to complex interactions or nonlinear relationships with other variables in the model. This interpretation suggests that further investigation is needed to understand the factors contributing to the non-monotonic behavior.
    

### model1: GLM

GLM: Generalized Linear Model   

**Interpretation**:      

It suggests a negative relationship between the predictor variable (x-axis) and the predicted outcome (y-axis). This means that as the value of the predictor variable increases, the predicted outcome decreases.     

The decreasing slope of the ALE plot suggests that the relationship between the predictor variable and the predicted outcome is becoming weaker as the value of the predictor variable increases. This is consistent with the decreasing ALE of y and predicted y values with increasing x-axis values.    

```{r}
ALE(model1, 
    "meals", 
    apipop_train, 
    apipop_train$api00
    )
```

### model2: CART   

CART: Classification And Regression Tree

**Interpretation**:      

It suggests that the relationship between the predictor variable and the predicted outcome is not linear. Instead, there may be discrete intervals or steps where the effect of the predictor variable on the predicted outcome changes.   

The step-like pattern in the ALE plot suggests that the relationship between the predictor variable and the predicted outcome may be due to different factors or conditions that affect the outcome. For example, there may be different subgroups or categories of observations that have different relationships between the predictor variable and the predicted outcome.    

```{r}
ALE(model2, 
    "not.hsg", 
    apipop_train, 
    apipop_train$api00
    )
```

### model3: XGBoost

XGBoost: eXtreme Gradient Boosting     

**Interpretation**:      

It suggests that the relationship between the predictor variable and the predicted outcome is not linear and may have some non-monotonic behavior.

The ups and downs in the ALE plot indicate that the relationship between the predictor variable and the predicted outcome may change direction or magnitude at certain values of the predictor variable. This non-monotonic behavior can arise due to complex interactions or nonlinear relationships between the predictor variable and other variables in the model.     

In this case, it may be useful to investigate the data further to identify the factors that contribute to the non-monotonic behavior. This could involve examining other variables that may interact with the predictor variable, or exploring subgroups or categories of observations that have different relationships with the predictor variable.    

```{r}
ALE(model3, 
    "meals", 
    apipop_train, 
    apipop_train$api00
    )
```

## d. Interaction plots   

- Compute the overall interaction H statistics     
- as well as the feature specific statistics for the **ELL variable** for all of the models.    
- Provide a table that includes as rows the predictors in the apipop_train data file and the H statistics for each model as columns.     
- For the ELL specific H statistics, provide an interaction plot per model.

### model1: GLM

GLM: Generalized Linear Model

**Interpretation**:      

- overal:
    - The overall interaction values for the Generalized Linear Model are all very small, with the largest value being 1.259390e-15 for the "cname" feature. This suggests that there is little to no interaction between the different features in the model, and that the features are largely independent in their influence on the target variable. In other words, the model does not detect any significant interactions between the features that would affect their contribution to the target variable. This result may indicate that the features have a simple linear relationship with the target variable or that the model is not complex enough to capture any non-linear relationships or interactions between the features.     
    
- Feature specific:
    - The output of the specific interaction for the Generalized Linear Model indicates the strength of interaction between two variables, in this case, the interaction between "stype" and "ell". The values of the coefficients represent the strength of the interaction. A coefficient close to zero indicates little or no interaction, while a large coefficient indicates a strong interaction between the two variables. Looking at the specific interaction coefficients, we can see that the interaction between "stype" and "ell" is very small, with a coefficient of 1.243676e-15. This suggests that there is little interaction between these two variables in predicting the target variable. Similarly, the coefficients for "cname:ell", "api00:ell", and "api99:ell" are all zero, indicating no interaction between these variables. On the other hand, we can see a relatively strong interaction between "meals" and the target variable "ell" (coefficient of 3.420603e-16), as well as between "not.hsg" and "ell" (coefficient of 2.050319e-15) and between "some.col" and "ell" (coefficient of 4.600026e-15).
    
    

```{r}
Interaction_overall(model1, 
                    apipop_train, 
                    apipop_train$api00
                    )
```

```{r}
Interaction_feature_specific(model1, 
                             apipop_train, 
                             apipop_train$api00,
                             "ell"
                             )
```

### model2: CART

CART: Classification and Regression Tree

**Interpretation**:      

- overal:
    - The ".feature" column lists the names of the independent variables (predictors or features) in the dataset, which are represented as character strings. The ".interaction" column shows the importance of each feature in the CART model, as measured by the Gini index. The Gini index is a measure of the impurity of a set of samples, and in the context of a CART model, it represents the extent to which a feature can be used to split the data into subsets that are more homogeneous with respect to the outcome variable (i.e., the dependent variable). The values in the ".interaction" column indicate the relative importance of each feature in the model. Higher values indicate that the feature is more important for splitting the data and improving the accuracy of the model, while lower values indicate that the feature is less important or may not be useful for splitting the data. In this particular output, the features "stype", "meals", and "not.hsg" have the highest importance scores, with values of 0.2715888, 0.2641148, and 0.1518510, respectively. The feature "avg.ed" has an importance score of 0.1200937, while the remaining features have importance scores of 0 or close to 0, indicating that they are not useful for splitting the data in this particular model.
    
- Feature specific:
    - The ".feature" column lists the names of the independent variables (predictors or features) in the dataset, which are represented as character strings, and the ".interaction" column shows the interaction (or importance) of each feature with a specific feature, "ell". In this specific interaction, the feature "ell" is used as the basis for splitting the data, and the values in the ".interaction" column represent the importance of each feature in splitting the data based on "ell". A value of 0 indicates that the feature is not useful for splitting the data based on "ell", while NaN (not a number) indicates that the feature was not used in the model for this specific interaction. Therefore, in this specific interaction, the features "stype", "meals", and "not.hsg" are useful for splitting the data based on "ell", with importance scores of 0. The features "avg.ed", "cname", "api00", "api99", "mobility", "pct.resp", "hsg", "some.col", "col.grad", "grad.sch", "full", "emer", "enroll", and "api.stu" are not useful for splitting the data based on "ell", as indicated by NaN importance scores. It's important to note that this is only a specific interaction of the CART model, and the importance of the features may differ when considering different interactions or the overall interaction of the model. Therefore, it's necessary to interpret the model's results and interactions in the context of the specific research question and dataset.

```{r}
Interaction_overall(model2, 
                    apipop_train, 
                    apipop_train$api00
                    )
```

```{r}
Interaction_feature_specific(model2, 
                             apipop_train, 
                             apipop_train$api00,
                             "ell"
                             )
```

### model3: XGBoost

XGBoost: eXtreme Gradient Boosting

**Interpretation**:      

- overal:
    - The ".feature" column lists the names of the independent variables (predictors or features) in the dataset, and the ".interaction" column shows the importance of each feature for the overall model. In this case, the values in the ".interaction" column indicate the relative contribution of each feature to the model's performance in predicting the outcome variable. According to the importance scores, "meals" is the most important feature for the model, with an importance score of 0.22003425. "pct.resp" and "avg.ed" also have relatively high importance scores of 0.11972590 and 0.10180762, respectively. Other features that are relatively important for the model include "cname", "ell", "not.hsg", and "api.stu", with importance scores ranging between 0.07118320 and 0.09305863. On the other hand, features such as "api00", "api99", "hsg", "some.col", "col.grad", "grad.sch", "emer", "enroll", and "mobility" have relatively low importance scores, suggesting that they have less influence on the model's predictions. It's worth noting that the importance scores reflect the relative contribution of each feature to the overall model and that they should be interpreted in the context of the specific research question and dataset. The importance scores may also vary depending on the hyperparameters and settings used in the XGBoost model.
    
- Feature specific:
    - This specific interaction output is showing the interaction between each pair of feature and the target variable, where the target variable is 'ell' (percentage of English language learners in a school). The values indicate the importance of the interaction between each feature and the target variable in predicting the target variable. A higher value indicates that the interaction between the two features is more important for predicting the target variable. For example, the interaction between 'cname' (county name) and 'ell' has a value of 0.17532633, which indicates that the county name has a strong interaction with the percentage of English language learners in a school. Similarly, the interaction between 'not.hsg' (percentage of adults in the community without a high school diploma) and 'ell' has a value of 0.19020011, which indicates that the percentage of adults in the community without a high school diploma has a strong interaction with the percentage of English language learners in a school.

```{r}
Interaction_overall(model3, 
                    apipop_train, 
                    apipop_train$api00
                    )
```

```{r}
Interaction_feature_specific(model3, 
                             apipop_train, 
                             apipop_train$api00,
                             "ell"
                             )
```

## e. Prediction performance    

Evaluate the prediction performance of each model using the test set that is included in the workspace (e.g. apipop_test).

### model1: GLM

GLM: Generalized Linear Model

```{r}
# prediction on test data
yhat <- predict(model1, s = model1, apipop_test)
# RMSE for test data
error.test <- yhat - apipop_test$api00
rmse.test <- sqrt(mean(error.test^2))
rmse.test
```

### model2: CART

CART: Classification And Regression Tree

```{r}
# prediction on test data
yhat <- predict(model1, s = model2, apipop_test)
# RMSE for test data
error.test <- yhat - apipop_test$api00
rmse.test <- sqrt(mean(error.test^2))
rmse.test
```

### model3: XGBoost

XGBoost: eXtreme Gradient Boosting

```{r}
# prediction on test data
yhat <- predict(model3, s = model1, apipop_test)
# RMSE for test data
error.test <- yhat - apipop_test$api00
rmse.test <- sqrt(mean(error.test^2))
rmse.test
```


## f.     

Considering the results for the tasks above, which model object belongs to which method? Explain your choice!

We could not get what the f part is asking for. So, we put an interpretation for each part.  