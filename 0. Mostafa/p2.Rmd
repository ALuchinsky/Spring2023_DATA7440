---
title: "p2"
author: "Mostafa"
date: "04/25/2023"
output:
    html_notebook: 
        toc: yes
        toc_float: yes
---

# Problem 2    

[50 points]

We will use the api data again for this problem, but this time we are interested in predicting the api test scores for 2000 (i.e. api00) based on a battery of school, teacher and parent information. This time the data and models are available in the apipopProb2.RData R workspace. Note the data sets here are different than for Problem 1.   

For this problem, you are given three models that aim at predicting **api00** (R objects model1 â€“ model3), using different methods (i.e., CART, XGBoost, OLS regression). Utilize the following interpretation techniques with the training data to learn more about how those models were trained. Specifically for each of the models you should:   

## Libraries

```{r}
rm(list = ls())

suppressPackageStartupMessages(library(randomForest)) # Random Forest
suppressPackageStartupMessages(library(pdp)) # pdp
suppressPackageStartupMessages(library(ggplot2)) # ggplot2
suppressPackageStartupMessages(library(iml)) # iml
suppressPackageStartupMessages(library(kableExtra)) # kableExtra
suppressPackageStartupMessages(library(knitr)) # needed for the kable function
suppressPackageStartupMessages(library(ICEbox))

suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(grid))
suppressPackageStartupMessages(library(gridExtra))
suppressPackageStartupMessages(library(lattice))
suppressPackageStartupMessages(library(xgboost))
suppressPackageStartupMessages(library(glmnet))
suppressPackageStartupMessages(library(rpart))
```

## Dataset

```{r}
load("./data/external/apipopProb2.RData")
```

## Functions

```{r}
PDP1var <- function(model1, var1) {

    pd1 <- partial(model1, 
                   pred.var = var1, 
                   plot = TRUE, 
                   rug = TRUE
                   ) 
    
    pd1a <- partial(model1, 
                    pred.var = var1, 
                    plot = TRUE, 
                    plot.engine="ggplot2"
                    ) 
    
    pd1a <- pd1a + ggtitle("ggplot2-based PDP") 
    
    grid.arrange(pd1, 
                 pd1a, 
                 ncol=2
                 )

}
```


```{r}
PDP2var <- function(model1, var1, var2) {
    
    pdp12 <-  partial(model1, 
                      pred.var = c(var1, var2), 
                      plot = TRUE, 
                      chull = TRUE, 
                      palette = "magma"
                      )
    pdp12
}
```


```{r}
ALE <- function(model1, feature1, data1, response1){
  
    predictor1 <- Predictor$new(model = model1, 
                                data = data1, 
                                y = response1
                                )
    
    pdp1  <- FeatureEffect$new(predictor1, 
                               feature=feature1, 
                               method="pdp"
                               ) 
    
    ale1  <- FeatureEffect$new(predictor1, 
                               feature=feature1
                               ) 
    
    grid.arrange(ale1$plot(), 
                 pdp1$plot(), 
                 ncol=2
                 )
}
```


## a. Partial Dependence plots 1-var   

Produce Partial Dependence plots for the most important predictor of each model.

### model1: Generalized Linear Model

```{r}
varImp(model1, scale = FALSE)
PDP1var(model1, "meals")
```

### model2: CART

```{r}
varImp(model2, scale = FALSE)
PDP1var(model2, "not.hsg")
```

### model3: eXtreme Gradient Boosting

```{r}
varImp(model3, scale = FALSE)
PDP1var(model3, "meals")
```

## b. Partial Dependence plots 2-var  

Produce Partial Dependence plots for the two most important predictors of each model (i.e., 3D plots or heatmaps or contour plots).

### model1: Generalized Linear Model

```{r}
varImp(model1, scale = FALSE)
PDP2var(model1, "meals", "ell")
```

### model2: CART

```{r}
varImp(model2, scale = FALSE)
PDP2var(model1, "not.hsg", "avg.ed")
```

### model3: eXtreme Gradient Boosting

```{r}
varImp(model3, scale = FALSE)
PDP2var(model1, "meals", "avg.ed")
```

## c. ALE plots     

- Produce Accumulated Local Effects plots for the most important predictor in each model.     
- How do these plots vary from those produced in part b? Given the correlation structure in the apipop_train data is it surprising or not that these plots differ or are similar to each other.

### model1: Generalized Linear Model

```{r}
varImp(model1, scale = FALSE)

ALE(model1, 
    "meals", 
    apipop_train, 
    apipop_train$api00
    )
```

### model2: CART

```{r}
varImp(model2, scale = FALSE)

ALE(model2, 
    "not.hsg", 
    apipop_train, 
    apipop_train$api00
    )
```

### model3: eXtreme Gradient Boosting

```{r}
varImp(model3, scale = FALSE)

ALE(model3, 
    "meals", 
    apipop_train, 
    apipop_train$api00
    )
```

## d. Interaction plots   

- Compute the overall interaction H statistics     
- as well as the feature specific statistics for the ELL variable for all of the models.    
- Provide a table that includes as rows the predictors in the apipop_train data file and the H statistics for each model as columns.     
- For the ELL specific H statistics, provide an interaction plot per model.

### model1: Generalized Linear Model

```{r}
predictor1<-Predictor$new(model1, data=apipop_train, y=apipop_train$api00)
interact <- Interaction$new(predictor1)
plot(interact)
```

### model2: CART

```{r}
predictor1<-Predictor$new(model2, data=apipop_train, y=apipop_train$api00)
interact <- Interaction$new(predictor1)
plot(interact)
```

### model3: eXtreme Gradient Boosting

```{r}
predictor1<-Predictor$new(model3, data=apipop_train, y=apipop_train$api00)
interact <- Interaction$new(predictor1)
plot(interact)
```


## e. Prediction performance    

Evaluate the prediction performance of each model using the test set that is included in the workspace (e.g. apipop_test).

### model1: Generalized Linear Model

```{r}
# prediction on test data
yhat <- predict(model1, s = model1, apipop_test)
# RMSE for test data
error.test <- yhat - apipop_test$api00
rmse.test <- sqrt(mean(error.test^2))
rmse.test
```

### model2: CART

```{r}
# prediction on test data
yhat <- predict(model1, s = model2, apipop_test)
# RMSE for test data
error.test <- yhat - apipop_test$api00
rmse.test <- sqrt(mean(error.test^2))
rmse.test
```

### model3: eXtreme Gradient Boosting

```{r}
# prediction on test data
yhat <- predict(model3, s = model1, apipop_test)
# RMSE for test data
error.test <- yhat - apipop_test$api00
rmse.test <- sqrt(mean(error.test^2))
rmse.test
```


## f. Conclusion    

Considering the results for the tasks above, which model object belongs to which method? Explain your choice!

- model1: Generalized Linear Model (GLM) typically produces a model object of class "glm", which contains information about the model coefficients, standard errors, and various goodness-of-fit measures.    

- model2: Classification and Regression Trees (CART) typically produces a model object of class "rpart", which contains information about the decision tree structure and splitting rules.    

- model3: eXtreme Gradient Boosting (XGBoost) typically produces a model object of class "xgb.Booster", which contains information about the gradient boosting model parameters, feature importance, and the learned ensemble of decision trees.
It is important to note that the specific properties of each model object may vary depending on the implementation and tuning choices made for each method.    
